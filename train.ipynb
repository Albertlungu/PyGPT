{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in /Users/albertlungu/.pyenv/versions/3.12.7/lib/python3.12/site-packages (from -r requirements.txt (line 1)) (4.4.1)\n",
      "Collecting matplotlib (from -r requirements.txt (line 2))\n",
      "  Using cached matplotlib-3.10.7-cp312-cp312-macosx_11_0_arm64.whl.metadata (11 kB)\n",
      "Requirement already satisfied: numpy in /Users/albertlungu/.pyenv/versions/3.12.7/lib/python3.12/site-packages (from -r requirements.txt (line 3)) (2.3.4)\n",
      "Requirement already satisfied: tqdm in /Users/albertlungu/.pyenv/versions/3.12.7/lib/python3.12/site-packages (from -r requirements.txt (line 4)) (4.67.1)\n",
      "Collecting jax (from -r requirements.txt (line 5))\n",
      "  Using cached jax-0.8.0-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting jaxlib (from -r requirements.txt (line 6))\n",
      "  Using cached jaxlib-0.8.0-cp312-cp312-macosx_11_0_arm64.whl.metadata (1.3 kB)\n",
      "Requirement already satisfied: filelock in /Users/albertlungu/.pyenv/versions/3.12.7/lib/python3.12/site-packages (from datasets->-r requirements.txt (line 1)) (3.20.0)\n",
      "Requirement already satisfied: pyarrow>=21.0.0 in /Users/albertlungu/.pyenv/versions/3.12.7/lib/python3.12/site-packages (from datasets->-r requirements.txt (line 1)) (22.0.0)\n",
      "Requirement already satisfied: dill<0.4.1,>=0.3.0 in /Users/albertlungu/.pyenv/versions/3.12.7/lib/python3.12/site-packages (from datasets->-r requirements.txt (line 1)) (0.4.0)\n",
      "Requirement already satisfied: pandas in /Users/albertlungu/.pyenv/versions/3.12.7/lib/python3.12/site-packages (from datasets->-r requirements.txt (line 1)) (2.3.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in /Users/albertlungu/.pyenv/versions/3.12.7/lib/python3.12/site-packages (from datasets->-r requirements.txt (line 1)) (2.32.5)\n",
      "Requirement already satisfied: httpx<1.0.0 in /Users/albertlungu/.pyenv/versions/3.12.7/lib/python3.12/site-packages (from datasets->-r requirements.txt (line 1)) (0.28.1)\n",
      "Requirement already satisfied: xxhash in /Users/albertlungu/.pyenv/versions/3.12.7/lib/python3.12/site-packages (from datasets->-r requirements.txt (line 1)) (3.6.0)\n",
      "Requirement already satisfied: multiprocess<0.70.19 in /Users/albertlungu/.pyenv/versions/3.12.7/lib/python3.12/site-packages (from datasets->-r requirements.txt (line 1)) (0.70.18)\n",
      "Requirement already satisfied: fsspec<=2025.10.0,>=2023.1.0 in /Users/albertlungu/.pyenv/versions/3.12.7/lib/python3.12/site-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets->-r requirements.txt (line 1)) (2025.10.0)\n",
      "Requirement already satisfied: huggingface-hub<2.0,>=0.25.0 in /Users/albertlungu/.pyenv/versions/3.12.7/lib/python3.12/site-packages (from datasets->-r requirements.txt (line 1)) (1.1.2)\n",
      "Requirement already satisfied: packaging in /Users/albertlungu/.pyenv/versions/3.12.7/lib/python3.12/site-packages (from datasets->-r requirements.txt (line 1)) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/albertlungu/.pyenv/versions/3.12.7/lib/python3.12/site-packages (from datasets->-r requirements.txt (line 1)) (6.0.3)\n",
      "Collecting contourpy>=1.0.1 (from matplotlib->-r requirements.txt (line 2))\n",
      "  Using cached contourpy-1.3.3-cp312-cp312-macosx_11_0_arm64.whl.metadata (5.5 kB)\n",
      "Collecting cycler>=0.10 (from matplotlib->-r requirements.txt (line 2))\n",
      "  Using cached cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib->-r requirements.txt (line 2))\n",
      "  Using cached fonttools-4.60.1-cp312-cp312-macosx_10_13_universal2.whl.metadata (112 kB)\n",
      "Collecting kiwisolver>=1.3.1 (from matplotlib->-r requirements.txt (line 2))\n",
      "  Using cached kiwisolver-1.4.9-cp312-cp312-macosx_11_0_arm64.whl.metadata (6.3 kB)\n",
      "Collecting pillow>=8 (from matplotlib->-r requirements.txt (line 2))\n",
      "  Using cached pillow-12.0.0-cp312-cp312-macosx_11_0_arm64.whl.metadata (8.8 kB)\n",
      "Collecting pyparsing>=3 (from matplotlib->-r requirements.txt (line 2))\n",
      "  Using cached pyparsing-3.2.5-py3-none-any.whl.metadata (5.0 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/albertlungu/.pyenv/versions/3.12.7/lib/python3.12/site-packages (from matplotlib->-r requirements.txt (line 2)) (2.9.0.post0)\n",
      "Collecting ml_dtypes>=0.5.0 (from jax->-r requirements.txt (line 5))\n",
      "  Using cached ml_dtypes-0.5.3-cp312-cp312-macosx_10_13_universal2.whl.metadata (8.9 kB)\n",
      "Collecting opt_einsum (from jax->-r requirements.txt (line 5))\n",
      "  Using cached opt_einsum-3.4.0-py3-none-any.whl.metadata (6.3 kB)\n",
      "Collecting scipy>=1.13 (from jax->-r requirements.txt (line 5))\n",
      "  Using cached scipy-1.16.3-cp312-cp312-macosx_14_0_arm64.whl.metadata (62 kB)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /Users/albertlungu/.pyenv/versions/3.12.7/lib/python3.12/site-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets->-r requirements.txt (line 1)) (3.13.2)\n",
      "Requirement already satisfied: anyio in /Users/albertlungu/.pyenv/versions/3.12.7/lib/python3.12/site-packages (from httpx<1.0.0->datasets->-r requirements.txt (line 1)) (4.11.0)\n",
      "Requirement already satisfied: certifi in /Users/albertlungu/.pyenv/versions/3.12.7/lib/python3.12/site-packages (from httpx<1.0.0->datasets->-r requirements.txt (line 1)) (2025.10.5)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/albertlungu/.pyenv/versions/3.12.7/lib/python3.12/site-packages (from httpx<1.0.0->datasets->-r requirements.txt (line 1)) (1.0.9)\n",
      "Requirement already satisfied: idna in /Users/albertlungu/.pyenv/versions/3.12.7/lib/python3.12/site-packages (from httpx<1.0.0->datasets->-r requirements.txt (line 1)) (3.11)\n",
      "Requirement already satisfied: h11>=0.16 in /Users/albertlungu/.pyenv/versions/3.12.7/lib/python3.12/site-packages (from httpcore==1.*->httpx<1.0.0->datasets->-r requirements.txt (line 1)) (0.16.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.2.0 in /Users/albertlungu/.pyenv/versions/3.12.7/lib/python3.12/site-packages (from huggingface-hub<2.0,>=0.25.0->datasets->-r requirements.txt (line 1)) (1.2.0)\n",
      "Requirement already satisfied: shellingham in /Users/albertlungu/.pyenv/versions/3.12.7/lib/python3.12/site-packages (from huggingface-hub<2.0,>=0.25.0->datasets->-r requirements.txt (line 1)) (1.5.4)\n",
      "Requirement already satisfied: typer-slim in /Users/albertlungu/.pyenv/versions/3.12.7/lib/python3.12/site-packages (from huggingface-hub<2.0,>=0.25.0->datasets->-r requirements.txt (line 1)) (0.20.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/albertlungu/.pyenv/versions/3.12.7/lib/python3.12/site-packages (from huggingface-hub<2.0,>=0.25.0->datasets->-r requirements.txt (line 1)) (4.15.0)\n",
      "Requirement already satisfied: six>=1.5 in /Users/albertlungu/.pyenv/versions/3.12.7/lib/python3.12/site-packages (from python-dateutil>=2.7->matplotlib->-r requirements.txt (line 2)) (1.17.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /Users/albertlungu/.pyenv/versions/3.12.7/lib/python3.12/site-packages (from requests>=2.32.2->datasets->-r requirements.txt (line 1)) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/albertlungu/.pyenv/versions/3.12.7/lib/python3.12/site-packages (from requests>=2.32.2->datasets->-r requirements.txt (line 1)) (2.5.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/albertlungu/.pyenv/versions/3.12.7/lib/python3.12/site-packages (from pandas->datasets->-r requirements.txt (line 1)) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/albertlungu/.pyenv/versions/3.12.7/lib/python3.12/site-packages (from pandas->datasets->-r requirements.txt (line 1)) (2025.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /Users/albertlungu/.pyenv/versions/3.12.7/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets->-r requirements.txt (line 1)) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /Users/albertlungu/.pyenv/versions/3.12.7/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets->-r requirements.txt (line 1)) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/albertlungu/.pyenv/versions/3.12.7/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets->-r requirements.txt (line 1)) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/albertlungu/.pyenv/versions/3.12.7/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets->-r requirements.txt (line 1)) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/albertlungu/.pyenv/versions/3.12.7/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets->-r requirements.txt (line 1)) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /Users/albertlungu/.pyenv/versions/3.12.7/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets->-r requirements.txt (line 1)) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /Users/albertlungu/.pyenv/versions/3.12.7/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets->-r requirements.txt (line 1)) (1.22.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in /Users/albertlungu/.pyenv/versions/3.12.7/lib/python3.12/site-packages (from anyio->httpx<1.0.0->datasets->-r requirements.txt (line 1)) (1.3.1)\n",
      "Requirement already satisfied: click>=8.0.0 in /Users/albertlungu/.pyenv/versions/3.12.7/lib/python3.12/site-packages (from typer-slim->huggingface-hub<2.0,>=0.25.0->datasets->-r requirements.txt (line 1)) (8.3.0)\n",
      "Using cached matplotlib-3.10.7-cp312-cp312-macosx_11_0_arm64.whl (8.1 MB)\n",
      "Using cached jax-0.8.0-py3-none-any.whl (2.9 MB)\n",
      "Using cached jaxlib-0.8.0-cp312-cp312-macosx_11_0_arm64.whl (55.0 MB)\n",
      "Using cached contourpy-1.3.3-cp312-cp312-macosx_11_0_arm64.whl (273 kB)\n",
      "Using cached cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Using cached fonttools-4.60.1-cp312-cp312-macosx_10_13_universal2.whl (2.8 MB)\n",
      "Using cached kiwisolver-1.4.9-cp312-cp312-macosx_11_0_arm64.whl (64 kB)\n",
      "Using cached ml_dtypes-0.5.3-cp312-cp312-macosx_10_13_universal2.whl (663 kB)\n",
      "Using cached pillow-12.0.0-cp312-cp312-macosx_11_0_arm64.whl (4.7 MB)\n",
      "Using cached pyparsing-3.2.5-py3-none-any.whl (113 kB)\n",
      "Using cached scipy-1.16.3-cp312-cp312-macosx_14_0_arm64.whl (20.9 MB)\n",
      "Using cached opt_einsum-3.4.0-py3-none-any.whl (71 kB)\n",
      "Installing collected packages: scipy, pyparsing, pillow, opt_einsum, ml_dtypes, kiwisolver, fonttools, cycler, contourpy, matplotlib, jaxlib, jax\n",
      "Successfully installed contourpy-1.3.3 cycler-0.12.1 fonttools-4.60.1 jax-0.8.0 jaxlib-0.8.0 kiwisolver-1.4.9 matplotlib-3.10.7 ml_dtypes-0.5.3 opt_einsum-3.4.0 pillow-12.0.0 pyparsing-3.2.5 scipy-1.16.3\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Imports successful\n",
      "\n",
      "Loading tokenizer...\n",
      "âœ“ Tokenizer loaded (vocab size: 1001)\n",
      "\n",
      "Loading training data...\n",
      "âœ“ Loaded 5158 training samples\n",
      "\n",
      "Initializing trainer...\n",
      "\n",
      "============================================================\n",
      "MODEL SUMMARY\n",
      "============================================================\n",
      "MODEL ARCHITECTURE SUMMARY\n",
      "============================================================\n",
      "Vocabulary Size:      1,001\n",
      "Embedding Dimension:  256\n",
      "Max Sequence Length:  512\n",
      "Number of Blocks:     4\n",
      "Number of Heads:      8\n",
      "FFN Hidden Dimension: 2048\n",
      "============================================================\n",
      "PARAMETER COUNTS\n",
      "============================================================\n",
      "Embedding Layer:           387,328 parameters\n",
      "Attention Layers:        1,048,576 parameters\n",
      "FeedForward Layers:      8,398,848 parameters\n",
      "Layer Normalization:         4,096 parameters\n",
      "Output Layer:              257,257 parameters\n",
      "------------------------------------------------------------\n",
      "TOTAL:                  10,096,105 parameters\n",
      "============================================================\n",
      "Model Size (float32): ~38.51 MB\n",
      "============================================================\n",
      "============================================================\n",
      "\n",
      "Starting training...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \r"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Shapes must be 1D sequences of concrete values of integer type, got (50, 263, JitTracer<~int32[]>, JitTracer<~int32[]>).\nIf using `jit`, try using `static_argnums` or applying `jit` to smaller subfunctions.\nThe error occurred while tracing the function fwd at /Users/albertlungu/Documents/GitHub/PyGPT/src/transformer/multi_head_attention.py:49 for jit. This concrete value was not available in Python because it depends on the value of the argument num_heads.\nThe error occurred while tracing the function fwd at /Users/albertlungu/Documents/GitHub/PyGPT/src/transformer/multi_head_attention.py:49 for jit. This concrete value was not available in Python because it depends on the value of the argument head_dim.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 68\u001b[39m\n\u001b[32m     65\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mStarting training...\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     66\u001b[39m train_time = time.time()\n\u001b[32m---> \u001b[39m\u001b[32m68\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     69\u001b[39m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     70\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m50\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     71\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcheckpoint_path\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43martifacts/training_logs/jax_training_latest.pkl\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     72\u001b[39m \u001b[43m    \u001b[49m\u001b[43msave_every\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m10\u001b[39;49m\n\u001b[32m     73\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     75\u001b[39m end_train = time.time() - train_time\n\u001b[32m     76\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mâœ“ Training complete! Time: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mend_train\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33ms\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/PyGPT/src/training/train.py:213\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self, epochs, batch_size, checkpoint_path, save_every)\u001b[39m\n\u001b[32m    210\u001b[39m target_tokens = batch_jax[:, \u001b[32m1\u001b[39m:]\n\u001b[32m    212\u001b[39m \u001b[38;5;66;03m# Forward + backward in ONE step (JAX magic!)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m213\u001b[39m loss, grads = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcompute_loss_and_grads\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_tokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_tokens\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    215\u001b[39m \u001b[38;5;66;03m# Update parameters\u001b[39;00m\n\u001b[32m    216\u001b[39m \u001b[38;5;28mself\u001b[39m.update_params(grads)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/PyGPT/src/training/train.py:133\u001b[39m, in \u001b[36mTrainer.compute_loss_and_grads\u001b[39m\u001b[34m(self, token_ids, targets)\u001b[39m\n\u001b[32m    130\u001b[39m stack_params = [block.get_params() \u001b[38;5;28;01mfor\u001b[39;00m block \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.transformer_stack.blocks]\n\u001b[32m    131\u001b[39m output_params = \u001b[38;5;28mself\u001b[39m.output_layer.get_params()\n\u001b[32m--> \u001b[39m\u001b[32m133\u001b[39m loss, grads = \u001b[43mjax\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalue_and_grad\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    134\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfull_fwd_and_loss\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    135\u001b[39m \u001b[43m    \u001b[49m\u001b[43margnums\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    136\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43membed_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstack_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    138\u001b[39m embed_grads, stack_grads, output_grads = grads\n\u001b[32m    140\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m loss, {\n\u001b[32m    141\u001b[39m     \u001b[33m'\u001b[39m\u001b[33membeddings\u001b[39m\u001b[33m'\u001b[39m: embed_grads,\n\u001b[32m    142\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mstack\u001b[39m\u001b[33m'\u001b[39m: stack_grads,\n\u001b[32m    143\u001b[39m     \u001b[33m'\u001b[39m\u001b[33moutput\u001b[39m\u001b[33m'\u001b[39m: output_grads\n\u001b[32m    144\u001b[39m }\n",
      "    \u001b[31m[... skipping hidden 10 frame]\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/PyGPT/src/training/train.py:116\u001b[39m, in \u001b[36mTrainer.compute_loss_and_grads.<locals>.full_fwd_and_loss\u001b[39m\u001b[34m(embed_params, stack_params, output_params)\u001b[39m\n\u001b[32m    113\u001b[39m     block_params = stack_params[i]\n\u001b[32m    114\u001b[39m     head_dim = \u001b[38;5;28mself\u001b[39m.embedding_layer.embedding_dim // \u001b[38;5;28mself\u001b[39m.num_heads\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m     current = \u001b[43mTransformerBlock\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfwd\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    117\u001b[39m \u001b[43m        \u001b[49m\u001b[43mblock_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    118\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcurrent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    119\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnum_heads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    120\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhead_dim\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    121\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43membedding_layer\u001b[49m\u001b[43m.\u001b[49m\u001b[43membedding_dim\u001b[49m\n\u001b[32m    122\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    124\u001b[39m logits = OutputLayer.fwd(output_params, current)\n\u001b[32m    125\u001b[39m loss = CrossEntropyLoss.fwd(logits, targets)\n",
      "    \u001b[31m[... skipping hidden 13 frame]\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/PyGPT/src/transformer/transformer_block.py:105\u001b[39m, in \u001b[36mTransformerBlock.fwd\u001b[39m\u001b[34m(params, x, num_heads, head_dim, embedding_dim)\u001b[39m\n\u001b[32m    102\u001b[39m residual_1 = x\n\u001b[32m    103\u001b[39m ln1_out = TransformerBlock.layer_norm(x, params[\u001b[33m'\u001b[39m\u001b[33mgamma_1\u001b[39m\u001b[33m'\u001b[39m], params[\u001b[33m'\u001b[39m\u001b[33mbeta_1\u001b[39m\u001b[33m'\u001b[39m])\n\u001b[32m--> \u001b[39m\u001b[32m105\u001b[39m attn_output = \u001b[43mMultiHeadAttention\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfwd\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    106\u001b[39m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mattn\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    107\u001b[39m \u001b[43m    \u001b[49m\u001b[43mln1_out\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    108\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_heads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    109\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhead_dim\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    110\u001b[39m \u001b[43m    \u001b[49m\u001b[43membedding_dim\u001b[49m\n\u001b[32m    111\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    112\u001b[39m after_attention = residual_1 + attn_output\n\u001b[32m    114\u001b[39m \u001b[38;5;66;03m# ==== Sublayer 2 ====\u001b[39;00m\n",
      "    \u001b[31m[... skipping hidden 13 frame]\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/PyGPT/src/transformer/multi_head_attention.py:70\u001b[39m, in \u001b[36mMultiHeadAttention.fwd\u001b[39m\u001b[34m(params, x, num_heads, head_dim, embedding_dim)\u001b[39m\n\u001b[32m     68\u001b[39m \u001b[38;5;66;03m# Reshape for multi-head (batch, seq_len, num_heads, head_dim)\u001b[39;00m\n\u001b[32m     69\u001b[39m batch_size, seq_len, _ = x.shape\n\u001b[32m---> \u001b[39m\u001b[32m70\u001b[39m Q = \u001b[43mQ\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseq_len\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_heads\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhead_dim\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# See readme for what .reshape does\u001b[39;00m\n\u001b[32m     71\u001b[39m K = K.reshape(batch_size, seq_len, num_heads, head_dim)\n\u001b[32m     72\u001b[39m V = V.reshape(batch_size, seq_len, num_heads, head_dim)\n",
      "    \u001b[31m[... skipping hidden 2 frame]\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.7/lib/python3.12/site-packages/jax/_src/numpy/array_methods.py:457\u001b[39m, in \u001b[36m_compute_newshape\u001b[39m\u001b[34m(arr, newshape)\u001b[39m\n\u001b[32m    455\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[32m    456\u001b[39m   newshape = [newshape]\n\u001b[32m--> \u001b[39m\u001b[32m457\u001b[39m newshape = \u001b[43mcore\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcanonicalize_shape\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnewshape\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[32m    458\u001b[39m neg1s = [i \u001b[38;5;28;01mfor\u001b[39;00m i, d \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(newshape) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(d) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28mint\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m d == -\u001b[32m1\u001b[39m]\n\u001b[32m    459\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(neg1s) > \u001b[32m1\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.7/lib/python3.12/site-packages/jax/_src/core.py:2017\u001b[39m, in \u001b[36mcanonicalize_shape\u001b[39m\u001b[34m(shape, context)\u001b[39m\n\u001b[32m   2015\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[32m   2016\u001b[39m   \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2017\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m _invalid_shape_error(shape, context)\n",
      "\u001b[31mTypeError\u001b[39m: Shapes must be 1D sequences of concrete values of integer type, got (50, 263, JitTracer<~int32[]>, JitTracer<~int32[]>).\nIf using `jit`, try using `static_argnums` or applying `jit` to smaller subfunctions.\nThe error occurred while tracing the function fwd at /Users/albertlungu/Documents/GitHub/PyGPT/src/transformer/multi_head_attention.py:49 for jit. This concrete value was not available in Python because it depends on the value of the argument num_heads.\nThe error occurred while tracing the function fwd at /Users/albertlungu/Documents/GitHub/PyGPT/src/transformer/multi_head_attention.py:49 for jit. This concrete value was not available in Python because it depends on the value of the argument head_dim."
     ]
    }
   ],
   "source": [
    "# PyGPT Training - All-in-One Cell\n",
    "# Just run this entire cell to train your model!\n",
    "\n",
    "# ============================================================\n",
    "# SETUP\n",
    "# ============================================================\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import pickle\n",
    "from datasets import load_dataset\n",
    "\n",
    "from src.training.train import Trainer\n",
    "from src.tokenizer.tokenizer_class import BPETokenizer\n",
    "\n",
    "print(\"âœ“ Imports successful\\n\")\n",
    "\n",
    "# ============================================================\n",
    "# LOAD TOKENIZER\n",
    "# ============================================================\n",
    "print(\"Loading tokenizer...\")\n",
    "with open(\"artifacts/tokenizer.pkl\", \"rb\") as f:\n",
    "    tokenizer = pickle.load(f)\n",
    "    tokenizer._ensure_vocab()\n",
    "\n",
    "print(f\"âœ“ Tokenizer loaded (vocab size: {tokenizer.vocab_size})\\n\")\n",
    "\n",
    "# ============================================================\n",
    "# LOAD TRAINING DATA\n",
    "# ============================================================\n",
    "print(\"Loading training data...\")\n",
    "max_lines = 1000\n",
    "dataset = load_dataset(\"tatsu-lab/alpaca\")\n",
    "train_data = dataset[\"train\"].select(range(max_lines))\n",
    "\n",
    "training_texts = []\n",
    "with open(\"tokenizer_training_data/alpaca_sample_utf8.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    for i, line in enumerate(f):\n",
    "        training_texts.append(line.strip())\n",
    "\n",
    "print(f\"âœ“ Loaded {len(training_texts)} training samples\\n\")\n",
    "\n",
    "# ============================================================\n",
    "# INITIALIZE TRAINER\n",
    "# ============================================================\n",
    "print(\"Initializing trainer...\")\n",
    "trainer = Trainer(\n",
    "    tokenizer=tokenizer,\n",
    "    user_input=training_texts,\n",
    "    lr=1e-4,\n",
    "    num_blocks=4,  # Stack 4 transformer blocks\n",
    "    num_heads=8    # 8 attention heads per block\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"MODEL SUMMARY\")\n",
    "trainer.print_model_summary()\n",
    "print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "# ============================================================\n",
    "# TRAIN MODEL\n",
    "# ============================================================\n",
    "print(\"Starting training...\\n\")\n",
    "train_time = time.time()\n",
    "\n",
    "trainer.train(\n",
    "    epochs=10,\n",
    "    batch_size=50,\n",
    "    checkpoint_path=\"artifacts/training_logs/jax_training_latest.pkl\",\n",
    "    save_every=10\n",
    ")\n",
    "\n",
    "end_train = time.time() - train_time\n",
    "print(f\"\\nâœ“ Training complete! Time: {end_train:.2f}s\\n\")\n",
    "\n",
    "# ============================================================\n",
    "# TEST GENERATION\n",
    "# ============================================================\n",
    "print(\"Testing text generation...\\n\")\n",
    "prompt = \"What is 5+5?\"\n",
    "generated_text = trainer.generate(prompt, max_length=50)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(f\"Prompt: {prompt}\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Generated: {generated_text}\")\n",
    "print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "# ============================================================\n",
    "# GENERATE FROM MULTIPLE PROMPTS\n",
    "# ============================================================\n",
    "print(\"Generating from multiple prompts...\\n\")\n",
    "prompts = [\n",
    "    \"Describe some of the benefits of a vegetarian diet.\",\n",
    "    \"What is the capital of France?\",\n",
    "    \"Explain machine learning in simple terms.\"\n",
    "]\n",
    "\n",
    "for prompt in prompts:\n",
    "    print(f\"Prompt: {prompt}\")\n",
    "    result = trainer.generate(\n",
    "        prompt,\n",
    "        max_length=50,\n",
    "        temperature=0.7,\n",
    "        top_k=40,\n",
    "        repetition_penalty=1.5\n",
    "    )\n",
    "    print(f\"Generated: {result}\")\n",
    "    print(\"-\" * 60 + \"\\n\")\n",
    "\n",
    "print(\"\\nðŸŽ‰ All done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.12.7",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
