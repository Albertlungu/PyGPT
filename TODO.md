# What's next?
- [x] Make sure there are no current filepath errors 

- [x] Split the encoding class into positional and embeddings

- [x] Make FFN model
- [ ] Make attention model
- [ ] Make the transformer block
- Use the following sources:
  - [Transformers from scratch](https://peterbloem.nl/blog/transformers)
  - [Attention Is All You Need](https://arxiv.org/pdf/1706.03762)
  - [Attention is Not All You Need: Pure Attention Loses Rank Doubly Exponentially with Depth](https://arxiv.org/pdf/2103.03404)
  - [Is Attention All What You Need? â€” An Empirical Investigation on Convolution-Based Active Memory and Self-Attention](https://arxiv.org/pdf/1912.11959)